{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:16:12.185960Z","iopub.execute_input":"2025-11-02T04:16:12.186145Z","iopub.status.idle":"2025-11-02T04:16:27.745566Z","shell.execute_reply.started":"2025-11-02T04:16:12.186127Z","shell.execute_reply":"2025-11-02T04:16:27.744821Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/openlm-research/open_llama_7b_v2\n\n‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/openlm-research/open_llama_7b_v2)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"openlm-research/open_llama_7b_v2\")","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:16:27.746955Z","iopub.execute_input":"2025-11-02T04:16:27.747180Z","iopub.status.idle":"2025-11-02T04:19:04.062896Z","shell.execute_reply.started":"2025-11-02T04:16:27.747161Z","shell.execute_reply":"2025-11-02T04:19:04.062018Z"}},"outputs":[{"name":"stderr","text":"2025-11-02 04:16:36.339469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762056996.511071      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762056996.557220      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43457ad3d4f74530bd458a5c7fd04947"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc106171f9c646909328171364400cff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2eb6eedfef342b88f5c9176d276543c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319eac28b80240a8b121a1bcb0a5728b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a3d377a5634efab7342259dc336b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0fc4798ab249a9b8b1d2eaf4118e4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f28942df5ed4aaba19b235c4663b31a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e8c7923c0641e7ab605052b16ae873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1266b7e92a5441cbf3c7092e596d754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/512k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98fb4289f0934beaaeb242d61379a442"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db5164e25da441e954cc2be74b31254"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\nDevice set to use cuda:0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b_v2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"openlm-research/open_llama_7b_v2\")","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:19:04.063748Z","iopub.execute_input":"2025-11-02T04:19:04.064403Z","iopub.status.idle":"2025-11-02T04:19:46.679216Z","shell.execute_reply.started":"2025-11-02T04:19:04.064377Z","shell.execute_reply":"2025-11-02T04:19:46.678672Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a320de6835eb4cafa8462e383129192e"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\n\n## v2 models\nmodel_path = 'openlm-research/open_llama_7b_v2'\n\n\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto',\n)\n\nprompt = 'Q: What is the largest animal?\\nA:'\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=32\n)\nprint(tokenizer.decode(generation_output[0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:21:29.037948Z","iopub.execute_input":"2025-11-02T04:21:29.038671Z","iopub.status.idle":"2025-11-02T04:22:24.960346Z","shell.execute_reply.started":"2025-11-02T04:21:29.038641Z","shell.execute_reply":"2025-11-02T04:22:24.959666Z"}},"outputs":[{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ff86123d5b4517af55ed5199e64dcf"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2532: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s>Q: What is the largest animal?\nA: A whale.\nQ: What is the smallest animal?\nA: A flea.\nQ: What is the most dangerous animal?\nA\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import re\nfrom transformers import GenerationConfig\n\ntriples = [\n    \"Kismet|directed_by|William Dieterle\",\n    \"Kismet|written_by|Edward Knoblock\",\n    \"Kismet|starred_actors|Marlene Dietrich\",\n    \"Flags of Our Fathers|directed_by|Clint Eastwood\",\n    \"Flags of Our Fathers|written_by|Paul Haggis\",\n    \"Flags of Our Fathers|has_genre|War\",\n    \"The Dark Horse|directed_by|Alfred E. Green\",\n    \"The Dark Horse|starred_actors|Bette Davis\",\n    \"The Dark Horse|release_year|1932\"\n]\n\nprompt_template = \"\"\"Convert the triple below into a natural English sentence.\n\nExample:\nInput: Casablanca|directed_by|Michael Curtiz\nOutput: Michael Curtiz directed Casablanca.\n\nNow verbalize this triple:\n{0}\nOutput:\"\"\"\n\ndef verbalize_one(triple):\n    prompt = prompt_template.format(triple)\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n    output_ids = model.generate(\n        input_ids,\n        max_new_tokens=64,\n        temperature=0.4,\n        top_p=0.9,\n        do_sample=False\n    )\n    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # Strip the prompt portion\n    text = text.replace(prompt, \"\").strip()\n    # Take first sentence\n    text = re.split(r\"[\\n\\r]\", text)[0].strip()\n    # Ensure punctuation\n    if not text.endswith(('.', '!', '?')):\n        text += '.'\n    return text\n\nfor t in triples:\n    print(verbalize_one(t))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:35:53.458183Z","iopub.execute_input":"2025-11-02T04:35:53.458757Z","iopub.status.idle":"2025-11-02T04:36:35.873040Z","shell.execute_reply.started":"2025-11-02T04:35:53.458732Z","shell.execute_reply":"2025-11-02T04:36:35.872331Z"}},"outputs":[{"name":"stdout","text":"William Dieterle directed Kismet.\nEdward Knoblock wrote Kismet.\nMarlene Dietrich starred in Kismet.\nClint Eastwood directed Flags of Our Fathers.\nPaul Haggis wrote Flags of Our Fathers.\nWar is the genre of Flags of Our Fathers.\nAlfred E. Green directed The Dark Horse.\nBette Davis starred in The Dark Horse.\nThe Dark Horse was released in 1932.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import re\nfrom transformers import GenerationConfig\n\ntriples = [\n    \"Kismet|directed_by|William Dieterle\",\n    \"Kismet|written_by|Edward Knoblock\",\n    \"Kismet|starred_actors|Marlene Dietrich\",\n    \"Flags of Our Fathers|directed_by|Clint Eastwood\",\n    \"Flags of Our Fathers|written_by|Paul Haggis\",\n    \"Flags of Our Fathers|has_genre|War\",\n    \"The Dark Horse|directed_by|Alfred E. Green\",\n    \"The Dark Horse|starred_actors|Bette Davis\",\n    \"The Dark Horse|release_year|1932\"\n]\n\nprompt_template = \"\"\"Convert the triple below into a natural English sentence.\n\nExample:\nInput: Casablanca|directed_by|Michael Curtiz\nOutput:Casablanca is directed by Michael Curtiz.\n\nNow verbalize this triple:\n{0}\nOutput:\"\"\"\n\ndef verbalize_one(triple):\n    prompt = prompt_template.format(triple)\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n    output_ids = model.generate(\n        input_ids,\n        max_new_tokens=64,\n        temperature=0.4,\n        top_p=0.9,\n        do_sample=False\n    )\n    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # Strip the prompt portion\n    text = text.replace(prompt, \"\").strip()\n    # Take first sentence\n    text = re.split(r\"[\\n\\r]\", text)[0].strip()\n    # Ensure punctuation\n    if not text.endswith(('.', '!', '?')):\n        text += '.'\n    return text\n\nfor t in triples:\n    print(verbalize_one(t))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T06:41:11.021208Z","iopub.execute_input":"2025-11-02T06:41:11.022111Z","iopub.status.idle":"2025-11-02T06:41:51.815647Z","shell.execute_reply.started":"2025-11-02T06:41:11.022084Z","shell.execute_reply":"2025-11-02T06:41:51.814800Z"}},"outputs":[{"name":"stdout","text":"Kismet is directed by William Dieterle.\nKismet is written by Edward Knoblock.\nMarlene Dietrich starred in Kismet.\nFlags of Our Fathers is directed by Clint Eastwood.\nFlags of Our Fathers is written by Paul Haggis.\nFlags of Our Fathers is a war movie.\nThe Dark Horse is directed by Alfred E. Green.\nBette Davis starred in The Dark Horse.\nThe Dark Horse was released in 1932.\n","output_type":"stream"}],"execution_count":40}]}